version: '3.8'
services:
  # HERE STARTS THE SETUP FOR ORION CONTEXT BROKER ITS UNDERLYING MONGODB AND
  # MONGO-EXPRESS AS DATABASE ADMINISTRATION INTERFACE
  orion:
    # Please, do only run one service of the following: orion, orion-ld, djeane
    # Sometimes the newest builds fail. Hence, we fixed a stable version that
    # probably will be updated once in a while
    image: fiware/orion:${ORION_VERSION:-3.1.0}
    hostname: orion
    depends_on:
        - mongodb
    networks:
        - fiware_backend
    ports:
        - "1026:1026"
    # Check carefully documentation for options:
    # https://fiware-orion.readthedocs.io/en/master/admin/cli/index.html
    # For in production mode please adjust these settings!
    environment:
      - ORION_LOG_LEVEL=${ORION_LOG_LEVEL:-ERROR}
      - ORION_LOG_FOR_HUMANS=${ORION_LOG_FOR_HUMANS:-TRUE}
      - ORION_MONGO_HOST=${ORION_MONGO_HOST:-mongodb}
      - ORION_MUTEX_POLICY=${ORION_MUTEX_POLICY:-None}
      - ORION_CONN_MEMORY=${ORION_CONN_MEMORY:-128}
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    #Limitation of logging
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 10m


  # MongoDB is the unterlying database engine
  mongodb:
  # We try to always provide the latest stable version
  # Check https://docs.mongodb.com/manual/release-notes/
    image: mongo:${MONGODB_VERSION:-4.4}
    hostname: mongo
    # Because within the same network the containers
    # are connected anyways port exposing is not required
    # --> only for external access
    ports:
    - "27017:27017"
    networks:
        - fiware_backend
    command: --bind_ip_all --quiet
    environment:
        - ALLOW_EMPTY_PASSWORD=${MONGODB_ALLOW_EMPTY_PASSWORD:-yes}
        - MONGODB_SYSTEM_LOG_VERBOSITY=${MONGODB_SYSTEM_LOG_VERBOSITY:-3}
        - MONGO_DATA_DIR=${MONGO_DATA_DIR:-/data/db}
        - MONGO_LOG_DIR=$MONGO_LOG_DIR:-/dev/null}
    volumes:
        - ${MONGODB_VOLUME_PATH:-/home/ebcadmin/fiware/data/mongodb}:/data/db
    deploy:
      placement:
        constraints: 
          - node.role == worker
          - node.labels.role == storage
          - node.hostname == tst-fiware-dev-storage1
    # Settings for log-files
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 5m
        

  # Mongo-express is a simple visualisation of mongoDB-instances
  mongo-express:
    image: mongo-express:0.54 # Check https://docs.mongodb.com/manual/release-notes/
    hostname: mongo-express
    # Because within the same network the containers
    # are connected port exposing is not required --> only for external access
    ports:
        - "8081:8081"
    networks:
        - fiware_backend
    environment:
      - ME_CONFIG_OPTIONS_EDITORTHEME=${ME_CONFIG_OPTIONS_EDITORTHEME:-ambiance}
      - ME_CONFIG_MONGODB_SERVER=${ME_CONFIG_MONGODB_SERVER:-mongodb}
      - ME_CONFIG_MONGODB_PORT=${ME_CONFIG_MONGODB_PORT:-27017}
      - ME_CONFIG_MONGODB_ENABLE_ADMIN=${ME_CONFIG_MONGODB_ENABLE_ADMIN:-true}
      - ME_CONFIG_MONGODB_AUTH_DATABASE=${ME_CONFIG_MONGODB_AUTH_DATABASE:-admin}
    # Adding dependencies
    # Settings for log-files
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 5m

  # HERE STARTS THE SETUP FOR TIMESERRIES COMPONENTS: QUUANTUMLEAP (NGSI-TIMESERIES-API),
  # A REDIS-CACHE FOR IT AND REDIS-COMMANDER AS ADMIN-INTERFACE TO REDIS
  # AS UNERDERLYING DATABASE FOR QUANTUMLEAP WE DEPLOY A CRATEDB CLUSTER WITH 3 DATA-NODES:
  # WE ACCESS THE NODES VIA A PROXY USING DOCKERFLOW-PROXY (https://proxy.dockerflow.com/), 
  # WHICH IS BASED ON HA-PROXY WITH A CONTINUOUS CONNECTION TO THE DOCKER-SWARM SOCKET!

  # As described, we are accessing the cratedb via a proxy using roundrobin as basic loadbalancing mechanism.
  # In used proxy service (https://proxy.dockerflow.com/) is based on HA-Proxy and will
  # reconfigureautomatically when scaling the database on multiple nodes based on container labels.
  cratedb-proxy-swarm-listener:
    image: dockerflow/docker-flow-swarm-listener:latest
    hostname: crate-proxy-swarm-listener
    networks:
      - fiware_backend
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    environment:
      - DF_NOTIFY_CREATE_SERVICE_URL=http://cratedb-proxy:8080/v1/docker-flow-proxy/reconfigure
      - DF_NOTIFY_REMOVE_SERVICE_URL=http://cratedb-proxy:8080/v1/docker-flow-proxy/remove
    deploy:
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints: [node.role == manager]

  cratedb-proxy:
    image: dockerflow/docker-flow-proxy:latest
    hostname: cratedb-proxy
    ports:
      - "4200:4200"
    networks:
      - fiware_backend
    environment:
      - SERVICE_NAME=cratedb-proxy
      - LISTENER_ADDRESS=cratedb-proxy-swarm-listener
      - MODE=swarm
      - BIND_PORTS=4200
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.role==manager
      update_config:
        parallelism: 1
        delay: 10s

  # Setup for cratedb as distributed time-series storage.
  # We use a three node setup here. All of them a eligable as master node.
  # In order to avoid a split brain scenario we want all of them up before selecting a master.
  # Only three nodes hold data which we place on our storage nodes 
  # via the docker labels.
  # If not all of nodes should hold data you can add coordiantor nodes. 
  # They are usually placed on one of the manager nodes of the swarm.
  # They only act as coordinators and do not hold any persistant data and Ã¡re therefore free to move around.
  
  # We always try to provide the latest stable version!
  # Please check release notes!
  # https://crate.io/docs/crate/guide/en/latest/deployment/containers/docker.html
  # Since Crate-DB uses the elastic stack under the hood for discovery we recommend to check:
  # https://www.elastic.co/guide/en/elastic-stack-get-started/current/get-started-docker.html
  cratedb-data:
    image: crate:${CRATEDB_VERSION:-4.5.0}
    labels:
      docker.service: cratedb
    hostname: "{{.Node.Hostname}}"
    networks:
      - fiware_backend
    volumes:
      - ${CRATEDB_VOLUME_PATH:-/home/ebcadmin/fiware/data/cratedb}:/data
    command: ["crate",
               "-Ccluster.name=${CRATEDB_CLUSTER_NAME:-YourDefaultClusterName}",
               "-Cnode.name=$$(hostname)",
               "-Cnode.data=true",
               "-Cnetwork.host=0.0.0.0",
               "-Cnetwork.publish_host=_eth0_",
               "-Cdiscovery.seed_hosts=cratedb-data",
               "-Cauth.host_based.enabled=false",
               "-Ccluster.initial_master_nodes=tst-fiware-dev-storage1",
               "-Chttp.cors.enabled=true",
               "-Chttp.cors.allow-origin='*'",
               "-Cgateway.expected_nodes=${CRATEDB_EXPECTED_NODES:-3}",
               "-Cgateway.recover_after_nodes=${CRATEDB_RECOVER_AFTER_NODES:-2}"]
    deploy:
      endpoint_mode: dnsrr
      mode: global
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.labels.role==storage
      labels:
        - com.df.notify=true
        - com.df.distribute=true
        - com.df.servicePath=/
        - com.df.port.1=4200
        - com.df.srcPort.1=4200
      update_config:
        parallelism: 1
        delay: 10s
    healthcheck:
      disable: false
    # Limiting of logging
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 50m
    environment:
       - CRATE_HEAP_SIZE=${CRATEDB_HEAP_SIZE:-4g}
       - MAX_MAP_COUNT=262144
       - ES_JAVA_OPTS="-Xms1g -Xmx1g"

  # NGSI-TIMESERIES-API aka QuantumLeap
  quantumleap:
    # We try to always provide the latest stable version
    image: orchestracities/quantumleap:${QL_VERSION:-0.8.1}
    hostname: quantumleap
    depends_on:
        - mongodb
        - orion
        - cratedb
    networks:
      - fiware_backend
    ports:
      - "8668:8668"
    deploy:
      replicas: 1
    environment:
      - CRATE_HOST=${QL_CRATEDB_HOST:-cratedb-proxy}
      - LOGLEVEL=${QL_LOG_LEVEL:-ERROR}
      - REDIS_HOST=${QL_REDIS_HOST:-redis-cache}
      - REDIS_PORT=${QL_REDIS_PORT:-6379}
      - CACHE_QUERIES=${QL_CACHE_QUERIES:-TRUE}
    logging:
        driver: "json-file"
        options:
            max-file: 5
            max-size: 1m

  redis-cache:
    image: redis:latest
    ports:
      - "6379:6379"
    networks:
      - fiware_backend
    #volumes:
    #  - quantumleap-cache:/data
    deploy:
      placement:
        constraints: [node.role == manager]

  redis-commander:
    image: rediscommander/redis-commander:latest
    depends_on:
      - redisdb
    environment:
      - REDIS_HOSTS=QuantumLeap:redis-cache:6379:1
    ports:
      - "8083:8081"
    networks:
      - fiware_backend    
    deploy:
      mode: replicated
      replicas: 1
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 5m


# HERE STARTS THE SETUP OF THE IOT-INTERFACE USING IOT-AGENTS AND MOSQUITTO
  mqtt-broker:
    image: eclipse-mosquitto:${MOSQUITTO_VERSION:-2.0.11}
    hostname: mqtt-broker
    ports:
      - "1883:1883"
#     # - "9001:9001"
    networks:
        - fiware_backend
    configs:
      - source: ${MOSQUITTO_CONFIG_NAME:-mosquitto.conf}
        target: mosquitto/config/mosquitto.conf
    deploy:
      replicas: 1


  iot-agent-ul:
    image: fiware/iotagent-ul:${IOTA_UL_VERSION:-1.18.0}
    hostname: iot-agent-ul
    networks:
      - fiware_backend
    depends_on:
      - mongodb
      - mqtt-broker
    ports:
      - "4061:4061"
      # - "7896:7896"
    environment:
      - IOTA_CB_HOST=${IOTA_UL_CB_HOST:-orion} # name of the context broker to update context
      - IOTA_CB_PORT=${IOTA_UL_CB_PORT:-1026} # port the context broker listens on to update context
      - IOTA_NORTH_PORT=${IOTA_UL_NORTH_PORT:-4061}
      - IOTA_REGISTRY_TYPE=${IOTA_UL_REGISTRY_TYPE:-mongodb} # Whether to hold IoT device info in memory or in a database
      - IOTA_LOG_LEVEL=${IOTA_UL_LOG_LEVEL:-ERROR} # The log level of the IoT Agent
      - IOTA_TIMESTAMP=${IOTA_UL_TIMESTAMP:-true} # Supply timestamp information with each measurement
      - IOTA_CB_NGSI_VERSION=${IOTA_UL_CB_NGSI_VERSION:-v2} # use NGSIv2 when sending updates for active attributes
      - IOTA_AUTOCAST=${IOTA_UL_AUTOCAST:-true} # Ensure Ultralight number values are read as numbers not strings
      - IOTA_MONGO_HOST=${IOTA_UL_MONGODB_HOST:-mongodb} # The host name of MongoDB or list of hosts in case mongodb is instanciated as replicaset
      - IOTA_MONGO_PORT=${IOTA_UL_MONGODB_PORT:-27017} # The port mongoDB is listening on
      - IOTA_MONGO_DB=${IOTA_UL_MONGODB_DB_NAME:-iotagentul} # The name of the database used in mongoDB
      - IOTA_MQTT_HOST=${IOTA_UL_MQTT_HOST:-mqtt-broker} # The host name of the MQTT Broker
      - IOTA_MQTT_PORT=${IOTA_UL_MQTT_PORT:-1883} # The port the MQTT Broker is listening on to receive topics
      - IOTA_MQTT_KEEPALIVE=${IOTA_UL_MQTT_KEEPALIVE:-60}
      - IOTA_PROVIDER_URL=${IOTA_UL_PROVIDER_URL:-http://iot-agent-ul:4061}
      - IOTA_MULTI_CORE=${IOTA_UL_MULTI_CORE:-true}
    deploy:
      replicas: 1
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 50m

# IoT-AGENT-JSON
  iot-agent-json:
    image: fiware/iotagent-json:${IOTA_JSON_VERSION:-1.17.2}
    hostname: iot-agent-json
    networks:
      - fiware_backend
    depends_on:
      - mongodb
      - mqtt-broker
    ports:
      - "4041:4041"
      - "7876:7896"
    environment:
      - IOTA_CB_HOST=orion # name of the context broker to update context
      - IOTA_CB_PORT=1026 # port the context broker listens on to update context
      - IOTA_NORTH_PORT=4041
      - IOTA_REGISTRY_TYPE=${IOTA_JSON_REGISTRY_TYPE:-mongodb} # Whether to hold IoT device info in memory or in a database
      - IOTA_LOG_LEVEL=${IOTA_JSON_LOG_LEVEL:-ERROR} # The log level of the IoT Agent
      - IOTA_TIMESTAMP=${IOTA_JSON_TIMESTAMP:-true} # Supply timestamp information with each measurement
      - IOTA_CB_NGSI_VERSION=${IOTA_JSON_CB_NGSI_VERSION:-v2} # use NGSIv2 when sending updates for active attributes
      - IOTA_AUTOCAST=${IOTA_JSON_AUTOCAST:-true} # Ensure Ultralight number values are read as numbers not strings
      - IOTA_MONGO_HOST=${IOTA_JSON_MONGODB_HOST:-mongodb} # The host name of MongoDB or list of hosts in case mongodb is instanciated as replicaset
      - IOTA_MONGO_PORT=${IOTA_JSON_MONGODB_PORT:-27017} # The port mongoDB is listening on
      - IOTA_MONGO_DB=${IOTA_JSON_MONGODB_DB_NAME:-iotagentjson} # The name of the database used in mongoDB
      - IOTA_MQTT_HOST=${IOTA_JSON_MQTT_HOST:-mqtt-broker} # The host name of the MQTT Broker
      - IOTA_MQTT_PORT=${IOTA_JSON_MQTT_PORT:-1883} # The port the MQTT Broker is listening on to receive topics
      - IOTA_MQTT_KEEPALIVE=${IOTA_JSON_MQTT_KEEPALIVE:-60}
      - IOTA_PROVIDER_URL=${IOTA_JSON_PROVIDER_URL:-http://iot-agent-json:4041}
      - IOTA_MULTI_CORE=${IOTA_JSON_MULTI_CORE:-true}
    deploy:
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    logging:
      driver: "json-file"
      options:
        max-file: 5
        max-size: 50m

networks:
  fiware_backend:

configs:
  mosquitto.conf:
    external: true
